{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The random variable X and Y have the following joint probability density ğ‘“ğ‘‹ğ‘Œ(ğ‘¥, ğ‘¦) = {ğ‘’^âˆ’ğ‘¥âˆ’ğ‘¦ 0 < ğ‘¥ < âˆ,0 < ğ‘¦ < âˆ , 0 ğ‘’ğ‘™ğ‘ ğ‘’ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ } \n",
    "\n",
    "## What is ğ‘ƒ(ğ‘‹ < ğ‘Œ) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"solution.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs(numbers,k):\n",
    "    \n",
    "    max_cycles= 0\n",
    "    add_iterations = len(numbers) -1\n",
    "\n",
    "    while(add_iterations>0):\n",
    "        max_cycles += add_iterations\n",
    "        add_iterations -= 1\n",
    "    \n",
    "    if (k > max_cycles):\n",
    "        print(\"Maximum no of pairs allowed is \", max_cycles)\n",
    "        return \n",
    "    \n",
    "    print (\"we will have\", k, \"pairs\")  \n",
    "    \n",
    "    count = 1\n",
    "   # matrix = []\n",
    "    for i in range(len(numbers)):\n",
    "        #matrix.append([])\n",
    "        for j in range(i+1,len(numbers)):\n",
    "            print(numbers[i],numbers[j])\n",
    "            if count == k:\n",
    "                return\n",
    "            else:\n",
    "                count +=1\n",
    "    \n",
    "     \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1,3,5]\n",
    "k = 3\n",
    "pairs(numbers,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findindices(data):\n",
    "    contact_dict = {}\n",
    "    email_dict = {}\n",
    "\n",
    "    for index, user in enumerate(data):                    \n",
    "        contact_group = contact_dict.get(user[1], set()) \n",
    "        email_group = email_dict.get(user[2], set())     \n",
    "        contact_group.add (index)\n",
    "        email_group.add (index)\n",
    "        contact_group.update (email_group)                  # Share info between the two groups\n",
    "        email_group.update (contact_group)                  \n",
    "        \n",
    "        for member in contact_group:\n",
    "            contact_dict[data[member][1]] = contact_group\n",
    "        \n",
    "        for member in email_group:                          \n",
    "            email_dict[data[member][2]] = email_group\n",
    "        \n",
    "\n",
    "\n",
    "    result = {tuple(x) for x in contact_dict.values()}      # use either contact_dict or email_dict will yield the same result\n",
    "    result = list(result) \n",
    "    print (result)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    " (\"username1\",\"phone_number1\", \"email1\"),\n",
    " (\"usernameX\",\"phone_number1\", \"emailX\"),\n",
    " (\"usernameZ\",\"phone_numberZ\", \"email1Z\"),\n",
    " (\"usernameY\",\"phone_numberY\", \"emailX\"),\n",
    " ]\n",
    "\n",
    "findindices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Forward propagation & Backward propagation for a three layers Neural Network. X,W and b can be random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help with forward propagation.\n",
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation_fn):\n",
    "    assert activation_fn == \"sigmoid\" or activation_fn == \"tanh\" or \\\n",
    "        activation_fn == \"relu\"\n",
    "\n",
    "    if activation_fn == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    elif activation_fn == \"tanh\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "\n",
    "    elif activation_fn == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    assert A.shape == (W.shape[0], A_prev.shape[1])\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, parameters, hidden_layers_activation_fn=\"relu\"):\n",
    "    A = X                           \n",
    "    caches = []                     \n",
    "    L = len(parameters) // 2        \n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(\n",
    "            A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)],\n",
    "            activation_fn=hidden_layers_activation_fn)\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(\n",
    "        A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],\n",
    "        activation_fn=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert AL.shape == (1, X.shape[1])\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(dA, Z):\n",
    "    A, Z = sigmoid(Z)\n",
    "    dZ = dA * A * (1 - A)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def tanh_gradient(dA, Z):\n",
    "    A, Z = tanh(Z)\n",
    "    dZ = dA * (1 - np.square(A))\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu_gradient(dA, Z):\n",
    "    A, Z = relu(Z)\n",
    "    dZ = np.multiply(dA, np.int64(A > 0))\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "# define helper functions that will be used in L-model back-prop\n",
    "def linear_backword(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    assert dA_prev.shape == A_prev.shape\n",
    "    assert dW.shape == W.shape\n",
    "    assert db.shape == b.shape\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation_fn):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation_fn == \"sigmoid\":\n",
    "        dZ = sigmoid_gradient(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
    "\n",
    "    elif activation_fn == \"tanh\":\n",
    "        dZ = tanh_gradient(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
    "\n",
    "    elif activation_fn == \"relu\":\n",
    "        dZ = relu_gradient(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(AL, y, caches, hidden_layers_activation_fn=\"relu\"):\n",
    "    y = y.reshape(AL.shape)\n",
    "    L = len(caches)\n",
    "    grads = {}\n",
    "\n",
    "    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n",
    "\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\n",
    "        \"db\" + str(L)] = linear_activation_backward(\n",
    "            dAL, caches[L - 1], \"sigmoid\")\n",
    "\n",
    "    for l in range(L - 1, 0, -1):\n",
    "        current_cache = caches[l - 1]\n",
    "        grads[\"dA\" + str(l - 1)], grads[\"dW\" + str(l)], grads[\n",
    "            \"db\" + str(l)] = linear_activation_backward(\n",
    "                grads[\"dA\" + str(l)], current_cache,\n",
    "                hidden_layers_activation_fn)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\n",
    "            \"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\n",
    "            \"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
